{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param definitions and options\n",
    "\n",
    "z_dim = 50\n",
    "b_size = 200\n",
    "max_iter = 1000\n",
    "step_size = 1e-3\n",
    "filter_size = 9\n",
    "max_rotation = .1\n",
    "fst_lyr_num_fltrs = 64\n",
    "scnd_lyr_num_fltrs = 64\n",
    "\n",
    "sample_batch = 1\n",
    "\n",
    "loss_to_use = 'l_1'; # options: 'l2_squared', 'l_1', 'cosine'\n",
    "#use_cosine = False\n",
    "use_leaky_relu = False\n",
    "use_tensorboard = True\n",
    "use_rotation = False\n",
    "use_warmstart = True\n",
    "save_model = True\n",
    "\n",
    "modelload_directory = './models/generator/test_zdim_50_8_more_var/'\n",
    "modelsave_directory = './models/generator/test_zdim_50_9_more_var/'\n",
    "tensorboard_dir = './logs/tensorboard/test_zdim_50_9_more_var'\n",
    "sample_dir = 'logs/test_zdim_50_9_more_var'\n",
    "\n",
    "# defining the sample directory if it does not exist\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def data2img(data):\n",
    "    shape = [28, 28, 1]\n",
    "    return np.reshape(data, [data.shape[0]] + shape)\n",
    "\n",
    "def grid_transform(x, size):\n",
    "    #a, t_b = split(x.shape[0])\n",
    "    #b = int(t_b)\n",
    "    a = 10\n",
    "    b = int(x.shape[0]/a)\n",
    "    h, w, c = size[0], size[1], size[2]\n",
    "    x = np.reshape(x, [a, b, h, w, c])\n",
    "    x = np.transpose(x, [0, 2, 1, 3, 4])\n",
    "    x = np.reshape(x, [a * h, b * w, c])\n",
    "    if x.shape[2] == 1:\n",
    "        x = np.squeeze(x, axis=2)\n",
    "    return x\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def weight_placeholder(shape, name=None):\n",
    "    return tf.placeholder(tf.float32, shape, name = name)\n",
    "\n",
    "def leaky_relu(x, alpha=0.2):\n",
    "    return tf.maximum(x * alpha, x)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_placeholder(shape, name = None):\n",
    "    return tf.placeholder(tf.float32, shape, name = name)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def return_normal(shape):\n",
    "    return np.float32(np.random.normal(loc=0.0, scale=0.1, size=shape))\n",
    "    \n",
    "\n",
    "def return_const(shape, c = .1):\n",
    "    return np.float32(np.full(shape = shape, fill_value = c))\n",
    "\n",
    "def return_unifrom(shape):\n",
    "    return np.float32(np.random.uniform(-1.0, 1.0, shape))\n",
    "\n",
    "def grad_placeholder(g):\n",
    "    g_placeholder = []\n",
    "    ctr = 0\n",
    "    for t in g:\n",
    "        #if use_warmstart:\n",
    "        #    graph = tf.get_default_graph()\n",
    "        #    g_placeholder.append(graph.get_tensor_by_name('grad_p'+str(ctr)+':0'))\n",
    "        #else:\n",
    "        g_placeholder.append(tf.placeholder(tf.float32, shape = t.get_shape().as_list(), name = 'grad_p'+str(ctr)))\n",
    "        ctr = ctr+1\n",
    "    return g_placeholder\n",
    "\n",
    "def feed_for_grad(g):\n",
    "    ctr = 0\n",
    "    feed={}\n",
    "    for t in g:\n",
    "        feed['grad_p'+str(ctr)+':0'] = t\n",
    "        ctr = ctr + 1\n",
    "    return feed\n",
    "\n",
    "def dist_d(a, b):\n",
    "    if loss_to_use == 'cosine':\n",
    "    #if use_cosine:\n",
    "        norms0 = tf.matmul(tf.sqrt(tf.reduce_sum(a**2,1, keep_dims=True)), tf.sqrt(tf.reduce_sum(b**2,1, keep_dims=True)), transpose_b=True) + 1e-5\n",
    "        return (1. - tf.matmul(a, b, transpose_b=True)/(norms0))*500.0\n",
    "    elif loss_to_use == 'l_1':\n",
    "        a = tf.expand_dims(a, 0)\n",
    "        b = tf.expand_dims(b, 1)\n",
    "        return tf.reduce_mean(tf.abs(a-b), 2)*500.\n",
    "    else:\n",
    "        a = tf.expand_dims(a, 0)\n",
    "        b = tf.expand_dims(b, 1)\n",
    "        return tf.reduce_mean((a-b)**2, 2)*500.\n",
    "\n",
    "#def initialize_uninitialized_vars(sess):\n",
    "#    from itertools import compress\n",
    "#    global_vars = tf.global_variables()\n",
    "#    is_not_initialized = sess.run([~(tf.is_variable_initialized(var)) \\\n",
    "#                                   for var in global_vars])\n",
    "#    not_initialized_vars = list(compress(global_vars, is_not_initialized))\n",
    "#\n",
    "#    if len(not_initialized_vars):\n",
    "#        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the session\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the generator\n",
    "\n",
    "if use_warmstart:\n",
    "    saver = tf.train.import_meta_graph(modelload_directory+'model.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(modelload_directory))\n",
    "    gen_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Generator\") \n",
    "    \n",
    "    gen_params_to_restore = sess.run(gen_params)\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "    print(gen_params)\n",
    "    #print(len(gen_params_to_restore))\n",
    "    #for t in gen_params_to_restore:\n",
    "    #    print(t.shape)\n",
    "    \n",
    "    warm_init_dict = {}\n",
    "    ctr = 0\n",
    "    for var in gen_params:\n",
    "        warm_init_dict[var.name] = gen_params_to_restore[ctr]\n",
    "        ctr = ctr + 1\n",
    "    #print(warm_init_dict)\n",
    "    sess = tf.InteractiveSession()\n",
    "    def return_warm_init(var_name):\n",
    "        value = warm_init_dict[var_name]\n",
    "        return tf.constant_initializer(value)\n",
    "\n",
    "def Generator(z, reuse = False):\n",
    "    with tf.variable_scope(\"Generator\", reuse=reuse):\n",
    "        if use_warmstart:\n",
    "            bs = tf.shape(z)[0]\n",
    "            fc1 = tf.layers.dense(z, 1024, \n",
    "                                 kernel_initializer = return_warm_init('Generator/dense/kernel:0'),\n",
    "                                 bias_initializer = return_warm_init('Generator/dense/bias:0'))\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "            fc2 = tf.layers.dense(fc1, 7 * 7 * 128, \n",
    "                                  kernel_initializer = return_warm_init('Generator/dense_1/kernel:0'),\n",
    "                                 bias_initializer = return_warm_init('Generator/dense_1/bias:0'))\n",
    "            fc2 = tf.reshape(fc2, tf.stack([bs, 7, 7, 128]))\n",
    "            fc2 = tf.nn.relu(fc2)\n",
    "            conv1 = tf.contrib.layers.conv2d_transpose(fc2, 64, [4, 4], [2, 2],\n",
    "                                                      weights_initializer = return_warm_init('Generator/Conv2d_transpose/weights:0'),\n",
    "                                                      biases_initializer = return_warm_init('Generator/Conv2d_transpose/biases:0'))\n",
    "            conv1 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.contrib.layers.conv2d_transpose(conv1, 1, [4, 4], [2, 2], activation_fn=tf.sigmoid,\n",
    "                                                      weights_initializer = return_warm_init('Generator/Conv2d_transpose_1/weights:0'),\n",
    "                                                      biases_initializer = return_warm_init('Generator/Conv2d_transpose_1/biases:0'))\n",
    "            conv2 = tf.reshape(conv2, tf.stack([bs, 784]))\n",
    "            \n",
    "        else:\n",
    "            bs = tf.shape(z)[0]\n",
    "            fc1 = tf.layers.dense(z, 1024)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "            fc2 = tf.layers.dense(fc1, 7 * 7 * 128)\n",
    "            fc2 = tf.reshape(fc2, tf.stack([bs, 7, 7, 128]))\n",
    "            fc2 = tf.nn.relu(fc2)\n",
    "            conv1 = tf.contrib.layers.conv2d_transpose(fc2, 64, [4, 4], [2, 2])\n",
    "            conv1 = tf.nn.relu(conv1)\n",
    "            conv2 = tf.contrib.layers.conv2d_transpose(conv1, 1, [4, 4], [2, 2], activation_fn=tf.sigmoid)\n",
    "            conv2 = tf.reshape(conv2, tf.stack([bs, 784]))\n",
    "            \n",
    "    return conv2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the discriminator\n",
    "\n",
    "W_conv1 = weight_placeholder([filter_size, filter_size, 1, fst_lyr_num_fltrs], name = 'W_conv1' )\n",
    "b_conv1 = bias_placeholder([fst_lyr_num_fltrs], name = 'b_conv1')\n",
    "\n",
    "W_conv2 = weight_placeholder([filter_size, filter_size, fst_lyr_num_fltrs, scnd_lyr_num_fltrs], name = 'W_conv2')\n",
    "b_conv2 = bias_placeholder([scnd_lyr_num_fltrs], name = 'b_conv2')\n",
    "\n",
    "if use_rotation:\n",
    "    angle = tf.placeholder(tf.float32, [1], name = 'angle')\n",
    "\n",
    "\n",
    "def Create_feed_dict():\n",
    "    feed = {}\n",
    "    if use_rotation:\n",
    "        feed[angle] = max_rotation*return_unifrom([1]);\n",
    "    feed[W_conv1] = return_normal([filter_size, filter_size, 1, fst_lyr_num_fltrs])\n",
    "    feed[b_conv1] = return_const([fst_lyr_num_fltrs], c = 0.0)\n",
    "    feed[W_conv2] = return_normal([filter_size, filter_size, fst_lyr_num_fltrs, scnd_lyr_num_fltrs])\n",
    "    feed[b_conv2] = return_const([scnd_lyr_num_fltrs], c = 0.0 )\n",
    "    #feed['z1:0'] = return_unifrom([b_size , z_dim])\n",
    "    #feed['z2:0'] = return_unifrom([b_size , z_dim])\n",
    "    return feed\n",
    "\n",
    "def Discriminator(x, angle = 0.0):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    \n",
    "    if use_rotation:\n",
    "        x_image = tf.contrib.image.rotate(x_image, angle)\n",
    "\n",
    "    ##  #first layer\n",
    "    if use_leaky_relu:\n",
    "        h_conv1 = leaky_relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    else:\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    \n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "    ##  #second layer\n",
    "    \n",
    "    if use_leaky_relu:\n",
    "        h_conv2 = leaky_relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    else:\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*scnd_lyr_num_fltrs])\n",
    "    #h_pool2_flat = tf.reshape(h_pool2, [-1, 14*14*64])\n",
    "    return h_pool2_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "    \n",
    "x1 = tf.placeholder(tf.float32, [None, 784], name = 'x1')\n",
    "x2 = tf.placeholder(tf.float32, [None, 784], name = 'x2')\n",
    "\n",
    "z1 = tf.placeholder(tf.float32, [None, z_dim], name = 'z1')\n",
    "z2 = tf.placeholder(tf.float32, [None, z_dim], name = 'z2')\n",
    "\n",
    "gz1 = Generator(z1)\n",
    "gz1 = tf.identity(gz1, name = 'gz1_n')\n",
    "gz2 = Generator(z2, reuse = True)\n",
    "gz2 = tf.identity(gz2, name = 'gz2_n')\n",
    "\n",
    "    \n",
    "if use_rotation:\n",
    "    \n",
    "    #angle = tf.placeholder(tf.float32, [1], name = 'angle')\n",
    "    \n",
    "    dx1 = Discriminator(x1, angle = angle)\n",
    "    dx2 = Discriminator(x2, angle = angle)\n",
    "\n",
    "\n",
    "    dz1 = Discriminator(gz1, angle = angle)\n",
    "    dz2 = Discriminator(gz2, angle = angle)\n",
    "    \n",
    "else:    \n",
    "    \n",
    "    dx1 = Discriminator(x1)\n",
    "    dx2 = Discriminator(x2)\n",
    "\n",
    "\n",
    "    dz1 = Discriminator(gz1)\n",
    "    dz2 = Discriminator(gz2)\n",
    "\n",
    "g_temp1 = dist_d(dx1, dx2)\n",
    "g_temp2 = dist_d(dz1, dz2)\n",
    "g_temp3 = dist_d(dx1, dz1)\n",
    "g_temp4 = dist_d(dx2, dz2)\n",
    "\n",
    "g_loss = tf.reduce_mean(g_temp4+ g_temp3- g_temp2 - g_temp1)\n",
    "\n",
    "\n",
    "\n",
    "if use_tensorboard:\n",
    "    #tf.summary.histogram(\"g_loss\", g_loss)\n",
    "    tf.summary.scalar('cost', g_loss)\n",
    "\n",
    "gen_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Generator\") \n",
    "print(gen_params)\n",
    "\n",
    "gen_grads = tf.gradients(g_loss, gen_params)\n",
    "print(gen_grads)\n",
    "\n",
    "grads_placeholder = grad_placeholder(gen_grads)#tf.placeholder(tf.float32, tf.shape(gen_grads), name = 'grads')\n",
    "\n",
    "print(grads_placeholder)\n",
    "print(gen_grads)\n",
    "\n",
    "#gen_train_op = tf.train.AdamOptimizer(learning_rate=step_size, beta1=0.5, beta2=0.9).minimize(g_loss, var_list=gen_params)\n",
    "\n",
    "#if use_warmstart:\n",
    "#    graph = tf.get_default_graph()\n",
    "#    gen_train_op = graph.get_operation_by_name(\"Adam1\") \n",
    "    \n",
    "gen_train_op = tf.train.AdamOptimizer(learning_rate=step_size,\n",
    "                                      beta1=0.5,\n",
    "                                      beta2=0.9, name = 'Adam2').apply_gradients(zip(grads_placeholder, gen_params))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a helper function that saves generated samples\n",
    "\n",
    "def Generate_samples_and_save(fixed_noise, iter = 0):\n",
    "    fake_samples = sess.run(gz1, feed_dict={z1:fixed_noise})\n",
    "    fake_samples = data2img(fake_samples)\n",
    "    fake_samples = grid_transform(fake_samples, [28, 28, 1])\n",
    "    fake_samples = np.squeeze(fake_samples)\n",
    "    #fake_samples = (255.99*fake_samples).astype('uint8')\n",
    "    plt.imsave(sample_dir+'/samples_'+str(iter)+'.png', fake_samples)\n",
    "    img = Image.open(sample_dir+'/samples_'+str(iter)+'.png').convert('LA')\n",
    "    img.save(sample_dir+'/samples_'+str(iter)+'.png')\n",
    "    return fake_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning\n",
    "\n",
    "\n",
    "fixed_noise = np.random.uniform(-1.0, 1.0, [120, z_dim])\n",
    "\n",
    "\n",
    "if use_tensorboard:\n",
    "    train_writer = tf.summary.FileWriter( tensorboard_dir, sess.graph)\n",
    "\n",
    "#if use_warmstart:\n",
    "#    initialize_uninitialized_vars(sess)\n",
    "#else:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost = []\n",
    "norm_grad = []\n",
    "for iter in range(max_iter):\n",
    "    for j in range(sample_batch):\n",
    "        feed = Create_feed_dict()\n",
    "        data1 = mnist.train.next_batch(b_size)[0]\n",
    "        data2 = mnist.train.next_batch(b_size)[0]\n",
    "        code1 = return_unifrom([b_size , z_dim])\n",
    "        code2 = return_unifrom([b_size , z_dim])\n",
    "        feed[x1] = data1\n",
    "        feed[x2] = data2\n",
    "        feed[z1] = code1\n",
    "        feed[z2] = code2\n",
    "        \n",
    "        if j==0:\n",
    "            cost_ , g_ = sess.run([g_loss, gen_grads], feed_dict = feed)\n",
    "            \n",
    "        else:\n",
    "            cost_j, g_j = sess.run([g_loss, gen_grads], feed_dict = feed)\n",
    "            cost_ = cost_ + cost_j\n",
    "            for i in range(len(g_)):\n",
    "                g_[i] = g_[i] + g_j[i]\n",
    "    \n",
    "    cost_ = (1.0/sample_batch)*cost_\n",
    "    norm_grad_i = 0\n",
    "    for t in g_:\n",
    "        t = (1.0/sample_batch)*t\n",
    "        norm_grad_i = norm_grad_i + np.sum(np.power(t,2))\n",
    "    norm_grad_i = np.sqrt(norm_grad_i)\n",
    "        \n",
    "    cost.append(cost_)\n",
    "    norm_grad.append(norm_grad_i)\n",
    "\n",
    "    _ = sess.run(gen_train_op, feed_dict=feed_for_grad(g_))    \n",
    "    \n",
    "    \n",
    "    if use_tensorboard:\n",
    "        #summary_writer = tf.train.SummaryWriter(FLAGS.logdir)\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='gen cost', simple_value=cost_)\n",
    "        summary.value.add(tag='grad norm', simple_value=norm_grad_i)\n",
    "        train_writer.add_summary(summary, iter)\n",
    "        #summary_writer.flush()\n",
    "\n",
    "    #if use_tensorboard:\n",
    "    #    merge = tf.summary.merge_all()\n",
    "    #    summary, _ = sess.run([merge, gen_train_op], feed_dict = feed)\n",
    "    #    train_writer.add_summary(summary, iter)\n",
    "    #else:\n",
    "    #    cost_, _ = sess.run([g_loss, gen_train_op], feed_dict = feed)\n",
    "    #    cost.append(cost_)\n",
    "    \n",
    "    if iter%10==0:\n",
    "        print('iter: ' + str(iter)+ ', cost: '+ str(cost[-1]) + ', grad norm: ' + str(norm_grad[-1]) )\n",
    "    if iter%100==0:\n",
    "        Generate_samples_and_save(fixed_noise, iter = iter)\n",
    "    \n",
    "Generate_samples_and_save(fixed_noise, iter = iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "if save_model:\n",
    "    saver_gen = tf.train.Saver(gen_params)\n",
    "    saver_gen.save(sess, modelsave_directory + 'model')\n",
    "    #saver = tf.train.Saver()\n",
    "    #tf.add_to_collection('bottle', h_fc1)\n",
    "    #saver.save(sess, modelsave_directory + 'model')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf",
   "language": "python",
   "name": "py36_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
